name: Py 2.7
on:
  pull_request:  # run on the PR base to check if the changes work before merging.
    # Note that this will also run upon any activity in the PR
    branches: [ 'master', 'develop' , 'testing' , 'mdbv2_compatibility', 'installer']
    types: [opened, synchronize]
    paths-ignore:
      - README.md  # don't run workflow when README changes
      - CHANGELOG.md # Should never be edited anyway
      - .gitignore
      - .github/**
  pull_request_target:  # run on PR target after closing the PR
    types:
      - closed
    paths-ignore:
        - README.md  # don't run workflow when README changes
        - CHANGELOG.md # Should never be edited anyway
        - .gitignore
        - .github/**
  push:
    branches:
      - testing
      - installer
  workflow_dispatch:

jobs:
  build:
    uses: ./.github/workflows/build-isf.yml
    with:
      py_version: 2.7
    # Py2 uses the defaults channel, which we currently dont have a license for
    if: false

  test:
    name: Test Py2.7
    needs: build  # assures there is a build of ISF
    # Assure test runs on whatever runner the build ran on
    # Note that runners are requested based on label, not on name
    # So the runner should have a label equal to its name
    runs-on: ${{ needs.build.outputs.runner_name }}
    defaults:
      run:
        shell: bash -l {0}
    # Py2 uses the defaults channel, which we currently dont have a license for
    if: false

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - name: Test ISF for Python 2.7
        run: |
          echo "------------ Preliminary setup for tests -------------"
          echo "Killing all dask processes on ${{ runner.name }} for user $USER (if they exist)"
          pkill -U $USER dask-worker
          pkill -U $USER dask-scheduler
          echo "Creating tests/logs/ directory..."
          mkdir -p ./tests/logs/
          if [ -f .coverage.${{ runner.name }}.* ]; then
            echo "Removing previous coverage files..."
            rm .coverage.${{ runner.name }}.*
          fi

          echo "------------ Setting up environment -------------"
          CONDA_TARGET_DIR="$HOME/anaconda_isf2.7"
          source $CONDA_TARGET_DIR/bin/activate
          echo "Using Python: $(which python)"
          export PYTHONPATH=$(pwd):$HOME
          echo $PYTHONPATH

          echo "------------ Configuring Dask -------------"
          name=${{ runner.name }}
          runner_number=$( echo $name | grep -Eo '[0-9]+$' )
          port_number=387${runner_number: -1}6
          bokeh_port=387${runner_number: -1}7
          echo "Launching Dask server on $name"
          echo "Using port $port_number and bokeh port $bokeh_port"

          export DASK_CONFIG=./config/dask_config.yml

          echo "------------ Running tests -------------"
          unset DISPLAY
          if [ ! -d "$HOME/tmp" ]; then
            mkdir $HOME/tmp
          fi
          export TMPDIR=$HOME/tmp
          (dask-scheduler --port=$port_number --bokeh-port=$bokeh_port --host=localhost --preload="mechanisms"> ./tests/logs/dask_scheduler_${{ github.run_id }}.log 2>&1) & \
          (dask-worker localhost:$port_number --nthreads 1 --nprocs 6 --memory-limit=100e15 > ./tests/logs/dask_workers_${{ github.run_id }}.log 2>&1) & \
          python -m pytest -rsx -vv --color=yes --dask_server_port $port_number tests/ -s || exit 1;
          rm -rf $HOME/tmp

      - name: Cleanup dask orphan processes
        if: ${{ always() }}
        run: |
          pkill -U $USER dask-scheduler
          pkill -U $USER dask-worker

      - name: Save test logs as artifacts
        uses: actions/upload-artifact@v3
        if: ${{ always() }} 
        with:
          name: ${{ github.run_id }}_logs
          path: |
            ./tests/logs/
# Installation of in-silico-framework followed by CI tests.
# Builds ISF locally on the MPINB-hosted runner and runs all tests
name: Py 2.7
# Controls when the workflow will run

on:
  push:
    branches: [ 'master' , 'testing' , 'mdbv2_compatibility']
    paths-ignore:
      - README.md  # don't run workflow when README changes
      - CHANGELOG.md # Should never be edited anyway
      - .gitignore
      - .github/**
  pull_request_target:  # only run on PR if it is closed, not while open
    types:
      - closed
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  
  build:
    uses: ./.github/workflows/build-isf.yml
    with:
      py_version: 2.7

  test:
    name: Test Py2.7
    needs: build  # assures there is a build of ISF
    # Assure test runs on whatever runner the build ran on
    # Note that runners are requested based on label, not on name
    # So the runner should have a label equal to its name
    runs-on: ${{ needs.build.outputs.runner_name }}
    defaults:
      run:
        shell: bash -l {0}

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - name: Test ISF for Python 2.7
        run: |
          echo "------------ Preliminary setup for tests -------------"
          echo "Killing all dask processes on ${{ runner.name }} for user $USER (if they exist)"
          pkill -U $USER dask-worker
          pkill -U $USER dask-scheduler
          echo "Creating tests/logs/ directory..."
          mkdir -p ./tests/logs/
          if [ -f .coverage.${{ runner.name }}.* ]; then
            echo "Removing previous coverage files..."
            rm .coverage.${{ runner.name }}.*
          fi

          echo "------------ Setting up environment -------------"
          CONDA_TARGET_DIR="$HOME/anaconda_isf2.7"
          source $CONDA_TARGET_DIR/bin/activate
          echo "Using Python: $(which python)"
          export PYTHONPATH=$(pwd):$HOME
          echo $PYTHONPATH

          echo "------------ Configuring Dask -------------"
          name=${{ runner.name }}
          runner_number=$( echo $name | grep -Eo '[0-9]+$' )
          port_number=387${runner_number: -1}6
          bokeh_port=387${runner_number: -1}7
          echo "Launching Dask server on $name"
          echo "Using port $port_number and bokeh port $bokeh_port"

          export DASK_CONFIG=./config/dask_config.yml

          echo "------------ Running tests -------------"
          unset DISPLAY
          if [ ! -d "$HOME/tmp" ]; then
            mkdir $HOME/tmp
          fi
          export TMPDIR=$HOME/tmp
          (dask-scheduler --port=$port_number --bokeh-port=$bokeh_port --host=localhost --preload="mechanisms"> ./tests/logs/dask_scheduler_${{ github.run_id }}.log 2>&1) & \
          (dask-worker localhost:$port_number --nthreads 1 --nprocs 6 --memory-limit=100e15 > ./tests/logs/dask_workers_${{ github.run_id }}.log 2>&1) & \
          python -m pytest -rsx -vv --color=yes --dask_server_port $port_number tests/ || exit 1;
          rm -rf $HOME/tmp

      - name: Cleanup dask orphan processes
        if: ${{ always() }}
        run: |
          pkill -U $USER dask-scheduler
          pkill -U $USER dask-worker

      - name: Save test logs as artifacts
        uses: actions/upload-artifact@v3
        if: ${{ always() }} 
        with:
          name: ${{ github.run_id }}_logs
          path: |
            ./tests/logs/
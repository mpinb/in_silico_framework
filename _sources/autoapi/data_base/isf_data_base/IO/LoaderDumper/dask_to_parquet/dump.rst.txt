
.. backlink:

:mod:`data_base` ❭ :mod:`~data_base.isf_data_base` ❭ :mod:`~data_base.isf_data_base.IO` ❭ :mod:`~data_base.isf_data_base.IO.LoaderDumper` ❭ :mod:`~data_base.isf_data_base.IO.LoaderDumper.dask_to_parquet` ❭ :mod:`~data_base.isf_data_base.IO.LoaderDumper.dask_to_parquet.dump`


.. title:

dump
====


.. py:function:: data_base.isf_data_base.IO.LoaderDumper.dask_to_parquet.dump(obj, savedir, schema=None, client=None, repartition=10000)

   Save a dask dataframe to one or more parquet files.

   One parquet file per partition is created.
   Each partition is written to a file named 'pandas\_to\_parquet.<n\_partitions>.<partition>.parquet'.
   The writing of these files is parallelized using the dask client if one is provided.

   In addition to the dask dataframe itself, meta information is saved in the form of a JSON file.

   .. seealso:: :py:func:`~data\_base.isf\_data\_base.IO.LoaderDumper.utils.save\_object\_meta` for saving meta information

   :Parameters: * **obj** (*dask.dataframe*) -- Dask dataframe to save
                * **savedir** (*str*) -- Directory where the parquet files will be stored
                * **client** (*dask.distributed.Client*) -- Dask client for parallellization.
                * **repartition** (*int*) -- If the original object has more than twice this amount of partitions, it will be repartitioned.
                  Otherwise, the object is saved according to its original partitioning.

   :returns: None

   .. seealso:: Each individual partitoin is saved using :py:meth:`~data\_base.isf\_data\_base.IO.LoaderDumper.dask\_to\_parquet.save\_helper`.


.. 
   Warning: we replace underscores with an escape backslash about 4 lines above to avoid having Sphinx interpret arguments as links.
   However, this may cause issues with code blocks or other literal text, and malform markdown tables
   Use with caution?
..
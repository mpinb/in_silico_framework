'''
the initialize function in this module is meant to be used to load in simulation results,
that are in the same format as roberts L6 simulations'''

from .. import IO
from .. import analyze

import os
import pandas as pd
import dask
import dask.dataframe as dd
import dask.bag as db
import numpy as np
import shutil
import tempfile
from .. import settings
from dask.diagnostics import ProgressBar
from ..IO.LoaderDumper import dask_to_csv, dask_to_msgpack
from ..IO import dask_wrappers
import model_data_base
from model_data_base.model_data_base import get_progress_bar_function


############################################
# Step one: create filelist containing paths to all soma voltage trace files
############################################
#this is covered in the IO.filelist module

############################################
#Step two: generate dask dataframe containing the voltagetraces
#This dataframe then contains the sim_trail_index
############################################

def read_voltage_traces_from_file(prefix, fname):
    '''reads a single voltage traces file as it is generated by roberts simulator.
    Returns a pandas.DataFrame'''
    full_fname = os.path.join(prefix, fname)
    data = np.loadtxt(full_fname, skiprows=1, unpack=True, dtype = 'float64')
    t = data[0]
    data = data[1:]
    index=[str(os.path.join(os.path.dirname(fname), str(index).zfill(6))) for index in range(len(data))] ##this will be the sim_trail_index
    df = pd.DataFrame(data, columns=t)
    df['sim_trail_index'] = index
    df.set_index('sim_trail_index', inplace = True)
    return df

def read_voltage_traces_by_filenames(prefix, fnames):
    '''takes list of filenames pointing to voltage trace files
    returned by roberts simulator, returns dask dataframe'''
    out = []
    for fname in sorted(fnames):
        out.append(dask.delayed(read_voltage_traces_from_file)(prefix, fname))
    out = dd.from_delayed(out, meta=out[0].compute())   
    #out = out.set_index('sim_trail_index') 
    return out

############################################
#Step three: read out the sim_trail_index from the soma voltage traces dask dataframe
#this is expensive and might be optimized
###########################################
#this is done directly in the _build function below

############################################
#Step four: generate metadata dataframe out of sim_trail_indices
############################################
def create_metadata(sim_trail_index):
    '''Generates metadata out of a pd.Series containing the sim_trail_indices'''
    def determine_zfill_used_in_simulation(x):
        path = x.sim_trail_index
        path, trailnr = os.path.split(path)
        glob.glob(os.path.join())
        
        
        
    def voltage_trace_file_list(x):
        '''returns part of the metadata dataframe.'''
        path = x.sim_trail_index
        #print path
        path, trailnr = os.path.split(path)
        #path, voltage_traces_file_name = os.path.split(path)
        voltage_traces_file_name = os.path.basename(path)
        voltage_traces_file_name = voltage_traces_file_name.split('_')[-1] + '_vm_all_traces.csv'
        return pd.Series({'path': path, 'trailnr': trailnr, 'voltage_traces_file_name': voltage_traces_file_name})
    
    
    def synaptic_file_list(x):
        '''returns part of the metadata dataframe.'''
        synapses_file_name = "simulation_run%04d_synapses.csv" % int(x.trailnr)
        cells_file_name = "simulation_run%04d_presynaptic_cells.csv" % int(x.trailnr)
        return pd.Series({'synapses_file_name': synapses_file_name, 'cells_file_name': cells_file_name}) 
    
    sim_trail_index = pd.DataFrame(dict(sim_trail_index = list(sim_trail_index)))
    path_trailnr = sim_trail_index.apply(voltage_trace_file_list, axis = 1)
    synaptic_files = path_trailnr.apply(synaptic_file_list, axis = 1)     
    sim_trail_index_complete = pd.concat((sim_trail_index, path_trailnr, synaptic_files), axis = 1)
    return sim_trail_index_complete

###########################################
#Step five: rewrite synapse and cell activation data to
#an csv format, that can be read by pandas and attach sim_trail_index to it
###########################################
def _max_commas(x, pathgenerator):
    '''for optimal performance, every single file should contain the same
    numer of columns. Therefore, before the data can be rewritten in
    an optimized form as csv, the maximum number of rows in all simulation
    trails in the project has to be determined.'''
    path = pathgenerator(x)
    with open(path, 'r') as f:
        text = f.read()
        text = text.replace('\t',',') #only , should be used
        commas_linewise = []
        for l in text.split('\n'):
            commas_linewise.append(l.count(','))
        max_commas = max(commas_linewise)
    return max_commas

def _convert_files_csv(prefix, prefix2, path, sim_trail, header, fname, max_commas):
    #make directories
    if not os.path.exists(os.path.join(prefix2, path)):
        os.makedirs(os.path.join(prefix2, path))
    #read file in and convert it
    with open(os.path.join(prefix, path, fname), 'r') as synFile:
        text = synFile.read()
    #remove leading or trailing whitespace
    text = text.strip()
    #only use , as seperator
    text = text.replace('\t',',') #only , should be used

    max_commas = max_commas + 1 #+1 because of one additional field (sim_trail)
    #every line needs to have the same number of fields
    text_with_commas = []
    for lv, l in enumerate(text.split('\n')):
        if lv == 0: #header
            if not header[-1] == ',': header = header + ','                
            for x in range(max_commas - header.count(',') + 1):
                header = header + str(x) + ','
            text_with_commas.append(header[:-1]) #remove last comma
        else: #data
            text_with_commas.append(sim_trail + ',' + l) 
    text = '\n'.join(text_with_commas)
    #write new file
    with open(os.path.join(prefix2, path, fname), 'w+') as synFile:
        synFile.write(text)
        #print os.path.join(prefix2, path, fname)
    #print(os.path.join(prefix2, path, fname))
    return 1  

class ConverterFabric:
    '''the main function of this class is to provide a function, which
    can be called with a single argument (a row of the metadata dataframe) to 
    convert the file specified in this line to an optimized format.
    
    This function is generated dynamically, based on the specified filetype ('cells' or 
    'synapses') and the number of collumns, that have to be expected at maximum. 
    
    The filetype can be set using the method set_filetype.
    The maximal number of commas is given to get_convert_fun. 
    
    get_convert_fun then returns a function, that can be applied to the metadata 
    dataframe.'''
    def __init__(self, mdb):
        self.mdb = mdb
    
    def set_filetype(self,filetype):
        self.filetype = filetype
    
    def get_pathgenerator(self):
        if self.filetype == 'cells':
            def pathgenerator(x):
                path = os.path.join(self.mdb['simresult_path'], x.path, x.cells_file_name)
                return path
            return pathgenerator
        if self.filetype == 'synapses':
            def pathgenerator(x):
                path = os.path.join(self.mdb['simresult_path'], x.path, x.synapses_file_name)
                return path
            return pathgenerator            
    
    def get_max_commas_fun(self):
        return lambda x: _max_commas(x, self.get_pathgenerator())
    
    def get_convert_fun(self, max_commas, dest_dir):
        '''
        returns a function, that takes a row of the 
        metadata dataframe, reads in the original simulation
        result and creates a corresponding file in the folder
        'dest_dir' in a pandas compatible format
        '''
        if self.filetype == 'cells':
            def convert_fun(x):
                path = x.path
                fname = x.cells_file_name
                sim_trail = x.sim_trail_index
                _convert_files_csv(self.mdb['simresult_path'], dest_dir, path, sim_trail, 'sim_trail_index,presynaptic_cell_type,cell_ID,', fname, max_commas)
                return 1 #return something, so dask will not complain
            
            return convert_fun
        if self.filetype == 'synapses':
            def convert_fun(x):
                path = x.path
                fname = x.synapses_file_name
                sim_trail = x.sim_trail_index
                _convert_files_csv(self.mdb['simresult_path'], dest_dir, path, sim_trail, 'sim_trail_index,synapse_type,synapse_ID,soma_distance,section_ID,section_pt_ID,dendrite_label,', fname, max_commas)
                return 1 #return something, so dask will not complain
            return convert_fun

def rewrite_data_in_fast_format(mdb):
    scheduler = settings.multiprocessing_scheduler
    metadata_dd = dd.from_pandas(mdb['metadata'], npartitions = settings.npartitions)
    myConvFab = ConverterFabric(mdb)
    
    print('Normalizing cell csv files: counting commas')
    myConvFab.set_filetype('cells')
    max_commas = metadata_dd.apply(myConvFab.get_max_commas_fun(), 
                                             axis = 1, 
                                             meta = int).compute(get = scheduler)#, meta = pd.Series({'max_commas': 'int64'}))
    max_commas = max(list(max_commas))
    print('Rewriting cell csv files to cache folder')    
    mdb['cells_cache_folder'] = cells_cache_folder = mdb.get_mkdtemp(suffix = 'cells_cache_folder')[0]
    metadata_dd.apply(myConvFab.get_convert_fun(max_commas, cells_cache_folder), 
                      axis = 1, meta = 1).compute(get = scheduler)
    
    print('Normalizing synapse csv files: counting commas')
    myConvFab.set_filetype('synapses')
    max_commas = metadata_dd.apply(myConvFab.get_max_commas_fun(), 
                                   axis = 1, 
                                   meta = 1).compute(get = scheduler)
    max_commas = max(list(max_commas))
    print('Rewriting synapse csv files to cache folder')  
    mdb['synapses_cache_folder'] = synapses_cache_folder = mdb.get_mkdtemp(suffix = 'synapses_cache_folder')[0]          
    metadata_dd.apply(myConvFab.get_convert_fun(max_commas, synapses_cache_folder), 
                      axis = 1, meta = 1).compute(get = scheduler)    

###########################################
#Step six: rewrite synapse and cell and voltage_trace data in a way
#that the data can be directly read by the dask.read_csv method
#
#This should not be necessary, however dask seems to be more robust, when it's
#read_csv method is used compared to the from_delayed method.
###########################################
#this is covered in _build_db_part3

###########################################
#Step seven: tidy up
##########################################



def _build_db_part1(mdb, repartition = False, force_calculation = False, dask_dumper = dask_to_csv):
    '''builds the metadata object and garants acces to the soma voltage traces.
    Only needs to be called once to put the necessary files in the tempdir'''
       
    print('building database ...')
    #make filelist of all soma-voltagetraces-files
    #mdb['file_list'] = IO.make_file_list(mdb['simresult_path'], 'vm_all_traces.csv')
    print('generate filelist ...')
    file_list = mdb.maybe_calculate('file_list', lambda: IO.make_file_list(mdb['simresult_path'], 'vm_all_traces.csv'))
    if len(file_list) == 0:
        raise ValueError("Did not find any '*vm_all_traces.csv'-files. Filelist empty. Abort initialization.")
    mdb.maybe_calculate('file_list', \
                        lambda: sorted(file_list, key = lambda x: os.path.dirname(x)), \
                        dumper = 'self', \
                        force_calculation = force_calculation)
    #read all soma voltage traces in dask dataframe
    print('generate voltage traces dataframe...')            
    mdb.maybe_calculate('voltage_traces_raw', \
                        lambda: read_voltage_traces_by_filenames(mdb['simresult_path'], mdb['file_list']), \
                        dumper = 'self',
                        force_calculation = force_calculation)
    #the indexes of this dataframe are stored for further use to identify the 
    #simulation trail
    print('Move voltage_traces locally')
    mdb.maybe_calculate('voltage_traces', \
                        lambda: mdb['voltage_traces_raw'], \
                        dumper = dask_dumper, 
                        force_calculation = force_calculation)
    del mdb['voltage_traces_raw']
    
    print('generate unambigous indices ...')            
    mdb.maybe_calculate('sim_trail_index',
                        lambda: mdb['voltage_traces'].index.compute(get = settings.multiprocessing_scheduler), 
                        dumper = 'self', \
                        force_calculation = force_calculation)
    #builds the metadata object, which connects the sim_trail indexes with the 
    #associated files
    print('generate metadata ...')        
    mdb.maybe_calculate('metadata', \
                        lambda: create_metadata(mdb['sim_trail_index']),\
                        dumper = 'self',\
                        force_calculation = force_calculation)                 


def _build_db_part2(mdb, dask_dumper = dask_to_csv):
    '''Rewrites synapse activation files for fast access and sets up access to them'''

    #rewrites the synapse and cell files in a way they can be acessed fast
    print('start rewriting synapse and cell activation data in optimized format')                
    rewrite_data_in_fast_format(mdb)     
    m = mdb['metadata']
    print('generate cell and synapse activation dataframes')
    index = list(mdb['sim_trail_index'])
    mdb['synapse_activation'] = dask_wrappers.read_csvs(mdb['synapses_cache_folder'], m.path, m.synapses_file_name).set_index('sim_trail_index', sorted = True, divisions = index)
    mdb['cell_activation'] = dask_wrappers.read_csvs(mdb['cells_cache_folder'], m.path, m.cells_file_name).set_index('sim_trail_index', sorted = True, divisions = index)
    
def _build_db_part3(mdb, dask_dumper = dask_to_csv):  
    '''rewrites synapse activation fiels again, this time with known divisions'''  
    print('Move synapse_activation to local database')    
    mdb.setitem(item = mdb['synapse_activation'], key = 'synapse_activation', dumper = dask_dumper)
    print('Move cell_activation to local database')    
    mdb.setitem(item = mdb['cell_activation'], key = 'cell_activation', dumper = dask_dumper)

def _tidy_up(mdb):
    print('Tidy up ...')
    basedir = mdb.basedir
    shutil.rmtree(os.path.join(basedir, mdb['synapses_cache_folder']))
    shutil.rmtree(os.path.join(basedir, mdb['cells_cache_folder']))
    del mdb['synapses_cache_folder']
    del mdb['cells_cache_folder']

def _regenerate_data(mdb):
    mdb['voltage_traces'] = IO.read_voltage_traces_by_filenames(mdb['simresult_path'], mdb['file_list'])
    _build_db_part2(mdb)   
    
def unique(input):
    output = []
    for x in input:
        if x not in output:
            output.append(x)
    return output

def sim_trial_index_generator(fname, len_data):
    dirname = os.path.dirname(fname)
    pid = os.path.basename(dirname).split('_')[-1]
    index = [str(os.path.join(dirname, pid + '_vm_all_traces.csv', str(index))) for index in range(len_data)]
    return index

#fname = '20160629-2316_16848/16848_apical_proximal_distal_rec_sites_ID_000_sec_038_seg_032_x_0.929_somaDist_920.7_vm_dend_traces.csv'
#sim_trial_index_generator(fname, 10)


def load_dendritic_voltage_traces_helper(mdb, suffix):
    m = mdb['metadata'] 
    if os.path.exists(os.path.join(mdb['simresult_path'], m.iloc[0].path, m.iloc[0].path.split('_')[-1] + suffix)):
        fnames = [os.path.join(x.path, x.path.split('_')[-1] + suffix) for index, x in m.iterrows()]
    elif os.path.exists(os.path.join(mdb['simresult_path'], m.iloc[0].path, 'seed_' + m.iloc[0].path.split('_')[-1] + suffix)):
        fnames = [os.path.join(x.path, 'seed_' + x.path.split('_')[-1] + suffix) for index, x in m.iterrows()]
    fnames = unique(fnames)
    ddf = read_voltage_traces_by_filenames(mdb['simresult_path'], fnames)
    return ddf

def load_dendritic_voltage_traces(mdb, repartition = False, dask_dumper = dask_to_csv, force_calculation = False):
    
    suffix = '_apical_proximal_distal_rec_sites_ID_000_sec_038_seg_032_x_0.929_somaDist_920.7_vm_dend_traces.csv'
    mdb.maybe_calculate('Vm_distal', \
                        lambda: load_dendritic_voltage_traces_helper(mdb, suffix), \
                        dumper = dask_dumper, \
                        force_calculation = force_calculation)
    suffix = '_apical_proximal_distal_rec_sites_ID_001_sec_025_seg_001_x_0.500_somaDist_198.1_vm_dend_traces.csv'
    mdb.maybe_calculate('Vm_proximal', \
                        lambda: load_dendritic_voltage_traces_helper(mdb, suffix), \
                        dumper = dask_dumper, \
                        force_calculation = force_calculation)    

#     suffix = '_apical_proximal_distal_rec_sites_ID_000_sec_038_seg_032_x_0.929_somaDist_920.7_vm_dend_traces.csv'    
#     ddf_distal = load_dendritic_voltage_traces_helper(mdb, suffix)      
#     suffix = '_apical_proximal_distal_rec_sites_ID_001_sec_025_seg_001_x_0.500_somaDist_198.1_vm_dend_traces.csv'
#     ddf_proximal = load_dendritic_voltage_traces_helper(mdb, suffix)
#     print('Move distal voltage traces to local database')
#     mdb.setitem(item = ddf_distal, key = 'Vm_distal', dumper = dask_to_csv, repartition = repartition)  
#     print('Move proximal voltage traces to local database')
#     mdb.setitem(item = ddf_proximal, key = 'Vm_proximal', dumper = dask_to_csv, repartition = repartition) 
    
        
from ..analyze.spike_detection import spike_detection
from ..analyze.burst_detection import burst_detection


def pipeline(mdb):
    #model_data_base.get_progress_bar_function()(): 
    #   like dask.diagnostics.ProgressBar, if model_data_base.settings.show_computation_progress,
    #   else empty context manager. This allows to hide the ProgressBar
    with dask.set_options(get = settings.multiprocessing_scheduler):
        with get_progress_bar_function()(): 
            load_dendritic_voltage_traces(mdb)    
            mdb['spike_times'] = spike_detection(mdb['voltage_traces'])
            mdb['burst_times'] = burst_detection(mdb['Vm_proximal'], mdb['spike_times'], burst_cutoff = -55)
        
def init(mdb, simresult_path, dask_dumper = dask_to_csv, voltage_traces = True, synapse_activation = True, synapse_activation_known_divisions = True, dendritic_voltage_traces = False, spike_times = False,  burst_times = False):
    with dask.set_options(get = settings.multiprocessing_scheduler):
        with get_progress_bar_function()(): 
            mdb['simresult_path'] = simresult_path  
            if voltage_traces:
                _build_db_part1(mdb, dask_dumper = dask_dumper)
                if synapse_activation:
                    _build_db_part2(mdb, dask_dumper = dask_dumper)
                    if synapse_activation_known_divisions:
                        _build_db_part3(mdb, dask_dumper = dask_dumper)
                        _tidy_up(mdb)
            if dendritic_voltage_traces:
                load_dendritic_voltage_traces(mdb, dask_dumper = dask_dumper)
            if spike_times:
                mdb['spike_times'] = spike_detection(mdb['voltage_traces'])
            if burst_times:
                mdb['burst_times'] = burst_detection(mdb['Vm_proximal'], mdb['spike_times'], burst_cutoff = -55)
        print('Initialization succesful.') 
    
    
    
    
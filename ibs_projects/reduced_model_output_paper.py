import Interface as I
import time
import msgpack
from .hot_zone import get_cell_object_from_hoc
import six


def get_surround_whiskers(whisker):
    '''arguments:
        whisker: str (in the format "A1", "C4", "Delta")
    returns:
        a list containing all surround whiskers for the provided whisker.'''
    layout = [['A1', 'A2', 'A3', 'A4'], ['B1', 'B2', 'B3', 'B4'],
              ['C1', 'C2', 'C3', 'C4'], ['D1', 'D2', 'D3', 'D4'],
              ['E1', 'E2', 'E3', 'E4']]
    greeks = ['Alpha', 'Beta', 'Gamma', 'Delta']
    greeks_lookup = dict(
        list(
            zip(greeks,
                [['A1', 'B1', 'Beta'], ['Alpha', 'Gamma', 'B1', 'C2'],
                 ['Beta', 'Delta', 'C1', 'D1'], ['Gamma', 'D1', 'E1']])))
    if whisker in greeks:
        sws = greeks_lookup[whisker]
        return sws

    row_index = [layout.index(l) for l in layout if whisker in l][0]
    row_range = [
        row_index - 1 if row_index - 1 >= 0 else 0,
        row_index + 1 if row_index + 1 <= 4 else 4
    ]

    arc_index = int(whisker[1]) - 1
    arc_range = [
        arc_index - 1 if arc_index - 1 >= 0 else 0,
        arc_index + 1 if arc_index + 1 <= 3 else 3
    ]

    sws = []
    for row in range(row_range[0], row_range[1] + 1):
        for arc in range(arc_range[0], arc_range[1] + 1):
            sws.append(layout[row][arc])
    if arc_index == 0:  # need to add greeks to surround
        sws.extend(greeks[row_index - 1 if row_index - 1 >= 0 else 0:row_index +
                          1])

    sws.remove(whisker)
    return sws


#=====================================================================#
# Code for setting up a feedforward reduced model network #
#=====================================================================#

# Step 1: generate network embedding using cortex in silico tools; see getting_started.ipynb


# Step 2: for each postsynaptic cell, get presynaptic cells (celltype and cellid)
def expand(mdb_or_folder
          ):  #lists contents without having to switch between keys and listdir
    try:
        return list(mdb_or_folder.keys())
    except AttributeError:
        return I.os.listdir(mdb_or_folder)


from six.moves import range as xrange


def chunker(seq, size):
    return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))


# def get_postsynaptic_cells_list_and_presynaptic_cells_dict(post_cells_dir):
#     presyn_cells = {}
#     postsyn_cells = expand(post_cells_dir)
#     for postsyn_cell in postsyn_cells:
#         #try:
#         with open(I.os.path.join(post_cells_dir, postsyn_cell, postsyn_cell+'.con'), 'r') as confile:
#             for line in confile:
#                 if not line.startswith('#') and not line.startswith('\n'):
#                     presyn_cells[line.split('\t')[1]] = line.split('\t')[0]
#     return postsyn_cells, presyn_cells
#         #except IOError:
#         #    pass


@I.dask.delayed
def get_presyn_cells_dict_chunk(postsyn_cells, post_cells_dir):
    out = {}
    for postsyn_cell in postsyn_cells:
        with open(
                I.os.path.join(post_cells_dir, postsyn_cell,
                               postsyn_cell + '.con'), 'r') as confile:
            for line in confile:
                if not line.startswith('#') and not line.startswith('\n'):
                    out[line.split('\t')[1]] = line.split('\t')[0]
    return out


def get_postsynaptic_cells_list_and_presynaptic_cells_dict(
        post_cells_dir, postsyn_cells=None, client=None):
    if not postsyn_cells:
        postsyn_cells = expand(post_cells_dir)
    delayeds = []
    for chunk in chunker(postsyn_cells, 100):
        delayeds.append(get_presyn_cells_dict_chunk(chunk, post_cells_dir))
    futures = client.compute(delayeds, optimize_graph=False)

    presyn_cells = {}
    for f in futures:
        presyn_cells.update(f.result())
    return postsyn_cells, presyn_cells


# Step 3: get soma distance lookup for each postsynaptic cell
# development: 20201118_compute_soma_distances.ipynb


def get_somadistance_lookup(cell,
                            segment_labels=[
                                'Dendrite', 'ApicalDendrite', 'Soma'
                            ]):
    '''returns label, soma distance of beginning and soma distance of end of each segment'''
    out = {}
    fun = I.sca.synanalysis.compute_distance_to_soma
    for lv, sec in enumerate(cell.sections):
        if not sec.label in segment_labels:
            continue
        out[lv] = sec.label, fun(sec, 0), fun(sec, 1)
    return out


# save_somadistance_lookup
def get_somadistance_lookup_parallel(post_cells_dir, post_cells=None):
    '''creates delayed objects to compute soma distances of all cells in post_cells_dir,
    which is generated by cortex in silico (see getting_started.ipynb)
    
    returns two lists: post_cell_ids, corresponding delayeds.
    
    Create the final dict by running:
    futures = client.compute(delayeds)
    results = client.gather(futures)
    final_dict = dict(zip(post_cells, results))
    '''

    @I.dask.delayed
    def _helper(cellID):
        hocpath = post_cells_dir + '/' + cellID + '/' + cellID + '.hoc'
        with I.silence_stdout:
            cell = get_cell_object_from_hoc(hocpath)
        return get_somadistance_lookup(cell)

    if not post_cells:
        post_cells = I.os.listdir(post_cells_dir)
    delayeds = [_helper(cellID) for cellID in post_cells]
    return post_cells, delayeds


#=====================================================================#
# Code for running a feedforward reduced model network #
#=====================================================================#


class FakeCell:
    pass


def generate_spiketimes(stim, cellNr_dict, tStop=245 + 50, mdb=None):
    '''hacks roberts single cell parser to generate pointcell activity defined in network param file'''
    simp = I.scp.NTParameterSet({'tStop': tStop})
    network_param = I.scp.build_parameters(mdb['network_param'].join(stim))
    for celltype, cn in six.iteritems(cellNr_dict):
        network_param.network[celltype].cellNr = cn
    network_mapper = I.scp.NetworkMapper(FakeCell(),
                                         network_param.network,
                                         simParam=simp)
    with I.silence_stdout:
        network_mapper._create_presyn_cells()
        network_mapper._activate_presyn_cells()
    return {
        k: [vv.spikeTimes for vv in v]
        for k, v in six.iteritems(network_mapper.cells)
    }


class ReducedModel:

    def __init__(self, kernel_dict, LUT, ISI_penalty, spatial_bin_size=50):
        self.kernel_dict = kernel_dict
        self.LUT = LUT
        self.ISI_penalty = ISI_penalty
        self.spatial_bin_size = spatial_bin_size

    def get_spatial_bin_by_soma_dist(self, soma_dist):
        pass

    def run(self, SAexc, SAinh, return_WNI=False, tStop=300, WNI_path=None):
        '''Apply the reduced model to synaptic input to get a list containing output spike times.
        SAexc, SAinh: numpy arrays containing spatiotemporally binned synaptic inputs
        return_WNI: False, 'EI_balance' or 'full_trace'
            full_trace returns a dataframe containing excitatory and inhibitory WNI values at all timepoints along with spike times. 
            EI_balance returns a dictionary containing the mean excitatory, inhibitory and total WNI value for the whole simulation 
            False returns spike times list only.            
        tStop: int'''
        s_exc = self.kernel_dict['s_exc']
        s_inh = self.kernel_dict['s_inh']
        t_exc = self.kernel_dict['t_exc']
        t_inh = self.kernel_dict['t_inh']

        LUT = self.LUT

        WNI_boundary = self.ISI_penalty

        SAinh_cumulative = [
        ]  # need to store past synapse activations so the temporal kernel can look back
        SAexc_cumulative = []

        exc_values = []
        inh_values = []
        wni_values = []

        spike_times = []  # list for recording output spikes
        for timebin in range(tStop):  # iterate through one timebin at a time
            ## get excitatory and inhibitory input, spatially binned for the CURRENT timebin
            SAexc_timebin = SAexc[:, timebin]
            SAinh_timebin = SAinh[:, timebin]

            ## apply spatial kernel to the current timebin
            SAexc_timebin = sum([o * s for o, s in zip(SAexc_timebin, s_exc)])
            SAinh_timebin = sum([o * s for o, s in zip(SAinh_timebin, s_inh)])

            # save the spatially filtered synapse activations for later timebins
            SAexc_cumulative.append(SAexc_timebin)
            SAinh_cumulative.append(SAinh_timebin)

            ## apply temporal kernel
            if timebin - 80 >= 0:  # if we can't look back 80 ms, then look back as far as possible
                SAexc_window = SAexc_cumulative[timebin - 79:timebin + 1]
                SAinh_window = SAinh_cumulative[timebin - 79:timebin + 1]
            else:
                SAexc_window = SAexc_cumulative[0:timebin + 1]
                SAinh_window = SAinh_cumulative[0:timebin + 1]

            SAexc_window = sum([
                o * s for o, s in zip(SAexc_window, t_exc[-len(SAexc_window):])
            ])
            SAinh_window = sum([
                o * s for o, s in zip(SAinh_window, t_inh[-len(SAinh_window):])
            ])

            ## get weighted net input and record it
            WNI = SAexc_window + SAinh_window

            if return_WNI == 'EI_balance' and timebin >= 80:  # exclude first 80 ms from the mean
                exc_values.append(SAexc_window)
                inh_values.append(SAinh_window)
                wni_values.append(WNI)
            else:
                wni_values.append(WNI)

            # apply ISI dependent WNI penalty
            if spike_times:  # if there have been spikes in the past
                last_spike_time = spike_times[-1]
                last_spike_interval = timebin - last_spike_time
                if last_spike_interval < 80:
                    penalty = WNI_boundary[-last_spike_interval]
                    WNI -= penalty

            ## get spike probability from WNI
            if WNI > LUT.index.max():
                spiking_probability = LUT[LUT.index.max()]
            elif WNI < LUT.index.min():
                spiking_probability = LUT[LUT.index.min()]
            else:
                spiking_probability = LUT[I.np.round(WNI)]

            ## will the cell spike or not?
            if spiking_probability > I.np.random.uniform() and timebin > 80:
                spike_times.append(timebin)

        if return_WNI:
            if return_WNI == 'EI_balance':
                WNI_return = {
                    'exc': I.np.mean(exc_values),
                    'inh': I.np.mean(inh_values),
                    'wni': I.np.mean(wni_values)
                }
            elif return_WNI == 'full_trace':
                WNI_return = wni_values
            else:
                raise ValueError(
                    "return_WNI must be False, EI_balance or full_trace")
            return spike_times, WNI_return
        elif WNI_path:
            with open(WNI_path, 'w') as f:
                I.cloudpickle.dump(wni_values, f)
            return spike_times
        else:
            return spike_times


class PreCell:

    def __init__(self, cellID, celltype):
        self.cellID = cellID
        self.celltype = celltype
        self.spike_times = []


class PostCell:
    '''A class for postsynaptic cells in reduced model networks.'''

    def __init__(self, cellID, celltype):
        '''cellID: int, from cortex in silico embedding
        celltype: str, e.g. L5tt_C2
        tStop: int'''
        self.cellID = cellID
        self.celltype = celltype
        self._EXC_times = [list() for lv in range(200)
                          ]  # range(rm.n_spatial_bins)]
        self._INH_times = [list() for lv in range(200)
                          ]  # range(rm.n_spatial_bins)]
        self.inh_scale = 1

    def add_connection(self, pre_cell, soma_dist, excinh, spatial_bin_size=50):
        '''Appends spike times from a presynaptic cell to the correct spatial bin of the postcell input array.
            pre_cell: PreCell object with spike times computed
            soma_dist: float
            excinh: str, exc or inh, reflecting presynaptic celltype'''
        assert isinstance(pre_cell.spike_times, list)
        spatial_bin = int(I.np.floor(float(soma_dist) / spatial_bin_size)
                         )  # self.rm.get_spatial_bin_by_soma_dist(soma_dist)
        if excinh == 'exc':
            self._EXC_times[spatial_bin].append(pre_cell.spike_times)
        if excinh == 'inh':
            self._INH_times[spatial_bin].append(pre_cell.spike_times)

    @staticmethod
    def _apply_release_probability_and_merge(times_array, relProb):
        '''Applies the release probability to activations at each synapse, then merges the synapse activation times list 
        by synapse soma distance.'''
        return [[
            l_
            for list_ in dist_
            for l_ in list_
            if I.np.random.rand() <= relProb
        ]
                for dist_ in times_array]

    @staticmethod
    def _get_SA_array(merged_times_array, tStop):
        bins = I.np.arange(0, tStop + 1)
        out = [I.np.histogram(l, bins)[0] for l in merged_times_array]
        return I.np.array(out)


#     def _get_spiking_output(self, return_WNI = False):   # not used to avoid reserialization of the reduced model
#         SAexc = self._apply_release_probability_and_merge(self._EXC_times, 0.6)
#         SAinh = self._apply_release_probability_and_merge(self._INH_times, 0.25)
#         SAexc = self._get_SA_array(SAexc, self.tStop)
#         SAinh = self._get_SA_array(SAinh, self.tStop)*self.inh_scale
#         return rm.run(SAexc, SAinh, tStop = self.tStop, return_WNI = return_WNI)

    @staticmethod
    def _get_spiking_output(_EXC_times,
                            _INH_times,
                            rm,
                            tStop,
                            inh_scale,
                            return_WNI=False,
                            WNI_path=None):
        '''rm should be a future scattered to the cluster to avoid reserialization of the reduced model'''
        _EXC_times = msgpack.unpackb(_EXC_times)
        _INH_times = msgpack.unpackb(_INH_times)
        SAexc = PostCell._apply_release_probability_and_merge(_EXC_times, 0.6)
        SAinh = PostCell._apply_release_probability_and_merge(_INH_times, 0.25)
        SAexc = PostCell._get_SA_array(SAexc, tStop)
        SAinh = PostCell._get_SA_array(SAinh, tStop) * inh_scale
        return rm.run(SAexc,
                      SAinh,
                      tStop=tStop,
                      return_WNI=return_WNI,
                      WNI_path=WNI_path)


class Network:
    '''A class for creating and simulating an embedded network of reduced model neurons.'''

    def __init__(self, presynaptic_cells_dict, postsynaptic_cells_list,
                 post_cells_dir, somadistance_dict):
        '''presynaptic_cells_dict: dict
            keys are cellIDs from cortex in silico embedding, values are celltypes in the format celltype_barrel, 
            created in get_postsynaptic_cells_list_and_presynaptic_cells_dict
        postsynaptic_cells_list: list
            containing cellIDs from cortex in silico embedding, basically all the subfolders in the post_cells_dir or a subset
            created in get_postsynaptic_cells_list_and_presynaptic_cells_dict
        post_cells_dir: str
            the filepath to the output of the cortex in silico network_realization.py, should end in /post_neurons
        somadistance_dict: dict, created by get_somadistance_lookup_parallel
            containing one key for each neuron morphology, storing dataframes with somadistances of all cell sections'''
        self.presynaptic_cells_dict = presynaptic_cells_dict
        self.postsynaptic_cells_list = postsynaptic_cells_list
        self.post_cells_dir = post_cells_dir
        self.somadistance_dict = somadistance_dict
        self.syn_files_by_cellid = {}
        self.con_files_by_cellid = {}
        self.cell_counts = I.pd.Series(list(presynaptic_cells_dict.values(
        ))).value_counts().to_dict(
        )  # counts the number of cells of each celltype, used in _activate_pre_cells
        self.pre_cells = I.defaultdict(
            list
        )  # created by _create_pre_syn_cells, stores PreCell objects by celltype
        self.pre_cells_by_id = {
        }  # created by _create_pre_syn_cells, stores PreCell objects by celltype
        self.post_cells = I.defaultdict(list)

        # self._read_syncon()

#     def _read_syncon(self): # don't need this anymore
#         post_cells_dir = self.post_cells_dir
#         for post_cell_ID in self.postsynaptic_cells_list:
#             con = I.scp.reader.read_functional_realization_map(I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID+'.con'))[0]
#             syn = I.scp.reader.read_synapse_realization(I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID+'.syn'))
#             self.con_files_by_cellid[post_cell_ID] = con
#             self.syn_files_by_cellid[post_cell_ID] = syn

    def _create_pre_syn_cells(self):
        for cellID, celltype in six.iteritems(self.presynaptic_cells_dict):
            pc = PreCell(cellID, celltype)
            self.pre_cells[celltype].append(pc)
            self.pre_cells_by_id[cellID] = pc

    def _create_post_syn_cells(self, celltype='L5tt'):
        for cellID in self.postsynaptic_cells_list:
            self.post_cells[cellID] = PostCell(cellID, celltype)

    def _wire(self, client, verbose=False):
        # read in syn-files remotely and extract soma distances of individual synapses remotely
        somadistance_dict_future = client.scatter(self.somadistance_dict)
        delayeds = []
        for c, cellID in enumerate(self.postsynaptic_cells_list):
            delayeds.append(
                _get_presyn_cells_with_synapse_distance_parallel(
                    post_cells_dir=self.post_cells_dir,
                    post_cell_ID=int(cellID),
                    somadistance_dict=somadistance_dict_future[cellID]))


#         delayeds = [Network._get_presyn_cells_with_synapse_distance_parallel(self.post_cells_dir,
#                                                                              cellID,
#                                                                              somadistance_dict_future)
#                     for cellID in self.postsynaptic_cells_list]
        futures = client.compute(delayeds)
        futures_distances_dict = dict(
            list(zip(self.postsynaptic_cells_list, futures)))
        # note: futures_distances_dict is a 'remote database' which allows to query the data
        # without the need to send all to the client node

        #
        if verbose:
            len_ = len(self.post_cells)
        for lv, c_post in enumerate(self.post_cells.values()):
            if verbose and lv % 10 == 0:
                print("{} of {} postsynaptic neurons.".format(lv + 1, len_))
            dict_ = futures_distances_dict[c_post.cellID].result()
            for c_pre, soma_distances in six.iteritems(dict_):
                c_pre_object = self.pre_cells_by_id[str(c_pre)]
                for soma_distance in soma_distances:
                    celltype = c_pre_object.celltype
                    celltype = celltype.split('_')[0]
                    excinh = 'exc' if celltype in I.excitatory else 'inh'  # self.pre_cells_by_id[c_pre]
                    c_post.add_connection(c_pre_object, soma_distance, excinh)

    def _activate_pre_cells(self,
                            stim=None,
                            tStop=300,
                            mdb=None,
                            network_param=None):
        '''mdb: a ModelDataBase with a key 'network_param', which is a folder containing network_param files.
        stim: name of the network param file in mdb['network_param']'''
        if network_param is not None:
            raise NotImplementedError(
                "currently, network activation needs to be specified with mdb and stim keyword."
            )
        network_mapper = generate_spiketimes(stim,
                                             self.cell_counts,
                                             tStop=tStop,
                                             mdb=mdb)
        for celltype in list(self.pre_cells.keys()):
            for pre_cell, spike_times in zip(self.pre_cells[celltype],
                                             network_mapper[celltype]):
                assert spike_times is not None
                del pre_cell.spike_times[:]
                pre_cell.spike_times.extend(spike_times)
                # pre_cell.spike_times = spike_times


@I.dask.delayed
def _get_presyn_cells_with_synapse_distance_parallel(post_cells_dir=None,
                                                     post_cell_ID=None,
                                                     somadistance_dict=None):
    #somadistance_dict = somadistance_dict.get() # resolve OpaqueCarrier to actual object
    post_cell_ID = str(post_cell_ID)
    presyn_cells_dict = I.defaultdict(list)
    # read_synfile, read_confile = syncon
    read_confile = I.scp.reader.read_functional_realization_map(
        I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID + '.con'))[0]
    read_synfile = I.scp.reader.read_synapse_realization(
        I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID + '.syn'))
    # using new somadistance lookup format, we can just look up our cellID
    # somadistance_dict = dist_mdb[post_cell_ID]
    # get the distances for all synapses
    for celltype in list(read_confile.keys()):
        for c, cell in enumerate(read_confile[celltype]):
            sec = read_synfile[celltype][c][0]
            secx = read_synfile[celltype][c][1]
            dist1 = somadistance_dict[sec][1]
            dist2 = somadistance_dict[sec][2]
            dist = dist1 + secx * (dist2 - dist1)
            presyn_cells_dict[cell[1]].append(dist)
    presyn_cells_dict = dict(presyn_cells_dict)
    return presyn_cells_dict


#     def _get_presyn_cells_with_synapse_distance(self, post_cell_ID):
#         post_cells_dir = self.post_cells_dir
#         presyn_cells_dict = I.defaultdict(list)

#         # load syn and con files
#         read_confile = I.scp.reader.read_functional_realization_map(I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID+'.con'))[0]
#         read_synfile = I.scp.reader.read_synapse_realization(I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID+'.syn'))

#         # using new somadistance lookup format, we can just look up our cellID
#         somadistance_dict = self.dist_mdb[post_cell_ID]

#         # get the distances for all synapses
#         for celltype in read_confile.keys():
#             for c, cell in enumerate(read_confile[celltype]):
#                 sec = read_synfile[celltype][c][0]
#                 secx = read_synfile[celltype][c][1]
#                 dist1 = somadistance_dict[sec][1]
#                 dist2 = somadistance_dict[sec][2]
#                 dist = dist1 + secx * (dist2 - dist1)
#                 presyn_cells_dict[cell[1]].append(dist)
#         presyn_cells_dict = dict(presyn_cells_dict)
#         return presyn_cells_dict


#===================#
# network balancing #
#===================#
# Step 1: fix ongoing EI balance issues from network embedding by scaling each cell's inhibitory kernel
def make_EI_balance_df(spike_times_ongoing,
                       WNIs_ongoing,
                       cells=None,
                       sim_time=10000):
    '''Sets up and returns the dataframe for scaling the inhibitory kernel of each cell in order to restore EI balance. Plot mean_WNI against mean_rate to determine your target WNI.
    spike_times_ongoing: dict, containing a key for each cell
    WNIs_ongoing: nested dict, containing a key for each cell, obtained by running the reduced model simulation with return_WNI = EI_balance
    sim_time: int, simulation length, default 10000'''
    EI_balance_df = I.pd.DataFrame(
        index=cells, columns=['mean_WNI', 'mean_rate', 'exc', 'inh'])

    meanrates = []
    meanexc = []
    meaninh = []
    meanwni = []
    for cellID in cells:
        spike_times = spike_times_ongoing[cellID]
        meanrates.append(len(spike_times) / ((sim_time) / 1000.))

        meanexc.append(WNIs_ongoing[cellID]['exc'])
        meaninh.append(WNIs_ongoing[cellID]['inh'])
        meanwni.append(WNIs_ongoing[cellID]['wni'])

    EI_balance_df['mean_rate'] = meanrates
    EI_balance_df['mean_WNI'] = meanwni
    EI_balance_df['exc'] = meanexc
    EI_balance_df['inh'] = meaninh

    return EI_balance_df


def calculate_scale_factors(EI_balance_df, target_WNI):
    '''Calculates the inhibitory spatial kernel scale factor for each cell, from a
    EI_balance_df: pandas dataframe, generated by make_EI_balance_df()
    target_WNI: float or int'''
    factors = []
    for cellID in EI_balance_df.index:
        factor = (target_WNI - EI_balance_df.loc[cellID, 'exc']
                 ) / EI_balance_df.loc[cellID, 'inh']
        factors.append(factor)

    EI_balance_df['inh_scale_factor'] = factors

    # E+x*I=t-E/
